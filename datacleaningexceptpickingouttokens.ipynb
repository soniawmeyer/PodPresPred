{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSG1S7eEStll",
        "outputId": "b2ed12b4-517e-416e-ab7d-71dfb2220492"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import os"
      ],
      "metadata": {
        "id": "IJGx6ZXZbkHN"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Election Prediction\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "N-9uwcjJa3-o"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "81P7u4UNcYLW"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "6Zrd8yCkcj5d",
        "outputId": "cdb0f24c-bac5-4446-a43a-ab5811394cd1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SparkContext master=local[*] appName=Election Prediction>"
            ],
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://1a2dc8d9d7c1:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Election Prediction</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading from podscribe through all the nested folders"
      ],
      "metadata": {
        "id": "KXJLMffXxQ4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import input_file_name\n",
        "\n",
        "# df = spark.read.option(\"recursiveFileLookup\", \"true\").text(\"/content/drive/Shareddrives/DATA228/Data/podscribe_app\")\n",
        "\n",
        "df = spark.read.option(\"recursiveFileLookup\", \"true\").text(\"/content/drive/Shareddrives/DATA228/Data/podscribe_app\") \\\n",
        "        .withColumn(\"file_name\", input_file_name())\n",
        "\n",
        "# # Extract the file names from the DataFrame\n",
        "# file_names = df.select(input_file_name().alias(\"file_path\")).distinct()\n",
        "\n",
        "# # Extract only the file names from the file paths\n",
        "# file_names = file_names.rdd.map(lambda row: os.path.basename(row[\"file_path\"])).collect()\n",
        "\n",
        "# # Print the list of file names\n",
        "# for file_name in file_names:\n",
        "#     print(file_name)"
      ],
      "metadata": {
        "id": "CBczBfUygfKY"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_names = df.select(input_file_name().alias(\"file_path\")).distinct()\n",
        "\n",
        "# Extract only the file names from the file paths\n",
        "file_names_rdd = file_names.rdd.map(lambda row: os.path.basename(row[\"file_path\"]))\n",
        "\n",
        "# Count the number of files\n",
        "file_count = file_names_rdd.count()\n",
        "\n",
        "# Print the count\n",
        "print(\"Number of files read:\", file_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-HzrjF-02Wm",
        "outputId": "5e534d4f-266c-45a6-bd1a-5e4fc5c5305d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of files read: 331\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "df = df.groupBy(\"file_name\").agg(\n",
        "    collect_list(\"value\").alias(\"value\")  # Collect all contents into a list\n",
        ")\n"
      ],
      "metadata": {
        "id": "Vtil8mn1lA0B"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn(\n",
        "    \"podcast_name_cleaned\",\n",
        "    regexp_replace(\n",
        "        regexp_replace(\n",
        "            regexp_extract(df[\"file_name\"], r\"/([^/]+?)(?:/[^/]+?\\.[a-z]+?$|$)\", 1),\n",
        "            \"%20\", \" \"\n",
        "        ),\n",
        "        \"[^a-zA-Z0-9\\\\s]\", \"\"\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "pMAGZ7jqjAZR"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.select(\"podcast_name_cleaned\",\"value\")\n"
      ],
      "metadata": {
        "id": "_o_haYDRgsmi"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn(\"cleaned_value\", concat_ws(\" \", df[\"value\"]))\n",
        "\n",
        "# Lowercase the text\n",
        "df = df.withColumn(\"cleaned_value\", lower(df[\"cleaned_value\"]))"
      ],
      "metadata": {
        "id": "CwJ2avcXil3P"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn(\"cleaned_value\", regexp_replace(df[\"cleaned_value\"], r\"\\[\\d{2}:\\d{2}:\\d{2}\\]|\\d{2}:\\d{2}\", \"\"))\n"
      ],
      "metadata": {
        "id": "HVfT-4AemOiP"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn(\"cleaned_value\", regexp_replace(df[\"cleaned_value\"], r\"\\b\\d+\\b\", \"\"))"
      ],
      "metadata": {
        "id": "1eik8q7im_ui"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn(\"cleaned_value\", regexp_replace(df[\"cleaned_value\"], r\"[^a-zA-Z0-9\\s]+\", \"\"))"
      ],
      "metadata": {
        "id": "pzC2N5HmoqUU"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn(\"cleaned_value\", trim(df[\"cleaned_value\"]))"
      ],
      "metadata": {
        "id": "yQ_4g2Nlou6C"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "\n",
        "# Apply the regex pattern to the 'transcript' column\n",
        "df = df.withColumn(\"cleaned_transcript\", F.regexp_replace(\"cleaned_value\", r'\\b\\d+m\\s*\\d+s|\\b\\d+s', \"\"))\n"
      ],
      "metadata": {
        "id": "DVUJF2NWrXqd"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
        "\n",
        "# Tokenize the cleaned transcripts column\n",
        "tokenizer = Tokenizer(inputCol=\"cleaned_transcript\", outputCol=\"tokens\")\n",
        "tokenized_df = tokenizer.transform(df)\n",
        "\n",
        "# Remove stopwords from the tokenized text\n",
        "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n",
        "filtered_df = remover.transform(tokenized_df)\n",
        "\n",
        "# Show the resulting DataFrame with the original data plus tokenized and filtered text\n",
        "df = filtered_df.select(\"podcast_name_cleaned\", \"cleaned_transcript\", \"filtered_tokens\")\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ffr1nSi2rBJi",
        "outputId": "9db69f36-976a-4b8d-df64-dc71e3cc2ef3"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+\n",
            "|podcast_name_cleaned|  cleaned_transcript|     filtered_tokens|\n",
            "+--------------------+--------------------+--------------------+\n",
            "|Bill OReillys No ...|samsung  tonight ...|[samsung, , tonig...|\n",
            "|Bill OReillys No ...| it is ryan here ...|[, ryan, question...|\n",
            "|Bill OReillys No ...|samsung  tonight ...|[samsung, , tonig...|\n",
            "|       Candace Owens| happy wednesday ...|[, happy, wednesd...|\n",
            "|       Candace Owens| all right guys h...|[, right, guys, h...|\n",
            "|       Candace Owens| alright everybod...|[, alright, every...|\n",
            "|       Candace Owens| alright right gu...|[, alright, right...|\n",
            "|Common Sense with...| common sense lin...|[, common, sense,...|\n",
            "|Common Sense with...| hes dan carlin a...|[, hes, dan, carl...|\n",
            "|Common Sense with...| hes dan carlin a...|[, hes, dan, carl...|\n",
            "|FiveThirtyEight P...| also im gonna ri...|[, also, im, gonn...|\n",
            "|FiveThirtyEight P...| the hunt for a n...|[, hunt, new, new...|\n",
            "|         Steve Deace| wake up america ...|[, wake, america,...|\n",
            "|         Steve Deace| and greetings ha...|[, greetings, hap...|\n",
            "|         Steve Deace| and greetings ha...|[, greetings, hap...|\n",
            "|   THE SAVAGE NATION| and now the worl...|[, worlds, exciti...|\n",
            "|The Clay Travis a...|bp  bp added more...|[bp, , bp, added,...|\n",
            "|The Clay Travis a...|bp  bp added more...|[bp, , bp, added,...|\n",
            "|The Clay Travis a...| luckyland casino...|[, luckyland, cas...|\n",
            "|The Clay Travis a...|bp  across americ...|[bp, , across, am...|\n",
            "+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import expr\n",
        "\n",
        "# Find the index of the first occurrence of \"biden\" or \"trump\" in the filtered_tokens column\n",
        "df = df.withColumn(\"biden_index\", expr(\"CASE WHEN array_contains(filtered_tokens, 'biden') THEN array_position(filtered_tokens, 'biden') ELSE -1 END\"))\n",
        "df = df.withColumn(\"trump_index\", expr(\"CASE WHEN array_contains(filtered_tokens, 'trump') THEN array_position(filtered_tokens, 'trump') ELSE -1 END\"))\n",
        "\n",
        "# Slice the filtered_tokens column based on the index of \"biden\" or \"trump\"\n",
        "df = df.withColumn(\"biden_context\", expr(\"slice(filtered_tokens, greatest(1, biden_index - 100), least(size(filtered_tokens), biden_index + 100))\"))\n",
        "df = df.withColumn(\"trump_context\", expr(\"slice(filtered_tokens, greatest(1, trump_index - 100), least(size(filtered_tokens), trump_index + 100))\"))\n",
        "\n",
        "# Show the DataFrame with the new columns\n",
        "df.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3kHf6L1UB8i",
        "outputId": "7829f896-f279-496d-a5f5-229999d41fcb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of rows\n",
        "num_rows = df.count()\n",
        "\n",
        "# Get the list of column names\n",
        "columns = df.columns\n",
        "\n",
        "# Get the number of columns\n",
        "num_cols = len(columns)\n",
        "\n",
        "# Print the shape of the DataFrame\n",
        "print(\"Shape of the DataFrame: {} rows, {} columns\".format(num_rows, num_cols))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPCd_0fsxXYe",
        "outputId": "c330ae6c-1feb-4a57-ac42-526874617f7c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the DataFrame: 331 rows, 3 columns\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "\n",
        "# Define a function to extract context tokens\n",
        "def extract_context(tokens, keywords):\n",
        "    contexts = []\n",
        "    for i, token in enumerate(tokens):\n",
        "        if any(keyword in token for keyword in keywords):\n",
        "            start_index = max(0, i - 100)\n",
        "            end_index = min(len(tokens), i + 101)\n",
        "            context = tokens[start_index:end_index]\n",
        "            contexts.append(context)\n",
        "    return contexts\n",
        "\n",
        "# Define a UDF to apply the extract_context function to each row of the DataFrame\n",
        "extract_context_udf = udf(lambda tokens: extract_context(tokens, [\"biden\", \"trump\"]), ArrayType(ArrayType(StringType())))\n",
        "\n",
        "# Apply the UDF to the filtered_tokens column and create a new column context_tokens\n",
        "df = df.withColumn(\"context_tokens\", extract_context_udf(df[\"filtered_tokens\"]))\n",
        "\n",
        "# Show the DataFrame with the new column\n",
        "df.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "id": "vrlEXHnhRoqw",
        "outputId": "10ec1637-fed0-4503-d6f5-8509678d000d"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "error",
          "ename": "PythonException",
          "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"<ipython-input-51-2fb11a668ad1>\", line 16, in <lambda>\n  File \"<ipython-input-51-2fb11a668ad1>\", line 9, in extract_context\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 174, in wrapped\n    return f(*args, **kwargs)\nTypeError: max() takes 1 positional argument but 2 were given\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-2fb11a668ad1>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Show the DataFrame with the new column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    943\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mBob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \"\"\"\n\u001b[0;32m--> 945\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m     def _show_string(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    974\u001b[0m                 )\n\u001b[1;32m    975\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_truncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"<ipython-input-51-2fb11a668ad1>\", line 16, in <lambda>\n  File \"<ipython-input-51-2fb11a668ad1>\", line 9, in extract_context\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 174, in wrapped\n    return f(*args, **kwargs)\nTypeError: max() takes 1 positional argument but 2 were given\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "\n",
        "# Define a function to extract context tokens\n",
        "def extract_context(tokens, keywords):\n",
        "    contexts = []\n",
        "    for i in range(len(tokens)):\n",
        "        token = tokens[i]\n",
        "        if any(keyword in token for keyword in keywords):\n",
        "            start_index = max(0, i - 100)\n",
        "            end_index = min(len(tokens), i + 101)\n",
        "            context = tokens[start_index:end_index]\n",
        "            contexts.append(context)\n",
        "    return contexts\n",
        "\n",
        "# Define a UDF to apply the extract_context function to each row of the DataFrame\n",
        "extract_context_udf = udf(lambda tokens: extract_context(tokens, [\"biden\", \"trump\"]), ArrayType(ArrayType(StringType())))\n",
        "\n",
        "# Apply the UDF to the filtered_tokens column and create a new column context_tokens\n",
        "df = df.withColumn(\"context_tokens\", extract_context_udf(df[\"filtered_tokens\"]))\n",
        "\n",
        "# Show the DataFrame with the new column\n",
        "df.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "id": "2nC6aB-My5oj",
        "outputId": "ff26a363-c2aa-40d2-ed27-101f7fef2de2"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "error",
          "ename": "PythonException",
          "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"<ipython-input-53-af22b2300dfe>\", line 17, in <lambda>\n  File \"<ipython-input-53-af22b2300dfe>\", line 10, in extract_context\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 174, in wrapped\n    return f(*args, **kwargs)\nTypeError: max() takes 1 positional argument but 2 were given\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-af22b2300dfe>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Show the DataFrame with the new column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    943\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mBob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \"\"\"\n\u001b[0;32m--> 945\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m     def _show_string(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    974\u001b[0m                 )\n\u001b[1;32m    975\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_truncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"<ipython-input-53-af22b2300dfe>\", line 17, in <lambda>\n  File \"<ipython-input-53-af22b2300dfe>\", line 10, in extract_context\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 174, in wrapped\n    return f(*args, **kwargs)\nTypeError: max() takes 1 positional argument but 2 were given\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf, col\n",
        "from pyspark.sql.types import ArrayType, StringType, IntegerType\n",
        "\n",
        "# Define a UDF to find the index of the first occurrence of a keyword in an array column\n",
        "find_index_udf = udf(lambda tokens, keyword: tokens.index(keyword) if keyword in tokens else -1, IntegerType())\n",
        "\n",
        "# Define a UDF to slice the array column based on the index of the keyword\n",
        "slice_array_udf = udf(lambda tokens, index: tokens[max(0, index - 100): min(len(tokens), index + 101)], ArrayType(StringType()))\n",
        "\n",
        "# Find the index of the first occurrence of \"biden\" or \"trump\" in the filtered_tokens column\n",
        "df = df.withColumn(\"biden_index\", find_index_udf(col(\"filtered_tokens\"), \"biden\"))\n",
        "df = df.withColumn(\"trump_index\", find_index_udf(col(\"filtered_tokens\"), \"trump\"))\n",
        "\n",
        "# Slice the filtered_tokens column based on the index of \"biden\" or \"trump\"\n",
        "df = df.withColumn(\"biden_context\", slice_array_udf(col(\"filtered_tokens\"), col(\"biden_index\")))\n",
        "df = df.withColumn(\"trump_context\", slice_array_udf(col(\"filtered_tokens\"), col(\"trump_index\")))\n",
        "\n",
        "# Show the DataFrame with the new columns\n",
        "df.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 882
        },
        "id": "iK2xa4riTVrb",
        "outputId": "6a177e51-7fef-4aad-b736-3bce5112e9ab"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `biden` cannot be resolved. Did you mean one of the following? [`filtered_tokens`, `context_tokens`, `cleaned_transcript`, `podcast_name_cleaned`].;\n'Project [podcast_name_cleaned#702, cleaned_transcript#732, filtered_tokens#752, context_tokens#1282, <lambda>(filtered_tokens#752, 'biden)#1358 AS biden_index#1359]\n+- Project [podcast_name_cleaned#702, cleaned_transcript#732, filtered_tokens#752, <lambda>(filtered_tokens#752)#1281 AS context_tokens#1282]\n   +- Project [podcast_name_cleaned#702, cleaned_transcript#732, filtered_tokens#752, <lambda>(filtered_tokens#752)#1206 AS context_tokens#1207]\n      +- Project [podcast_name_cleaned#702, cleaned_transcript#732, filtered_tokens#752, <lambda>(filtered_tokens#752)#1131 AS context_tokens#1132]\n         +- Project [podcast_name_cleaned#702, cleaned_transcript#732, filtered_tokens#752, <lambda>(filtered_tokens#752)#1056 AS context_tokens#1057]\n            +- Project [podcast_name_cleaned#702, cleaned_transcript#732, filtered_tokens#752, <lambda>(filtered_tokens#752)#981 AS context_tokens#982]\n               +- Project [podcast_name_cleaned#702, cleaned_transcript#732, filtered_tokens#752, <lambda>(filtered_tokens#752)#906 AS context_tokens#907]\n                  +- Project [podcast_name_cleaned#702, cleaned_transcript#732, filtered_tokens#752, <lambda>(filtered_tokens#752)#831 AS context_tokens#832]\n                     +- Project [podcast_name_cleaned#702, cleaned_transcript#732, filtered_tokens#752]\n                        +- Project [podcast_name_cleaned#702, value#699, cleaned_value#728, cleaned_transcript#732, tokens#740, UDF(tokens#740) AS filtered_tokens#752]\n                           +- Project [podcast_name_cleaned#702, value#699, cleaned_value#728, cleaned_transcript#732, UDF(cleaned_transcript#732) AS tokens#740]\n                              +- Project [podcast_name_cleaned#702, value#699, cleaned_value#728, regexp_replace(cleaned_value#728, \\b\\d+m\\s*\\d+s|\\b\\d+s, , 1) AS cleaned_transcript#732]\n                                 +- Project [podcast_name_cleaned#702, value#699, trim(cleaned_value#724, None) AS cleaned_value#728]\n                                    +- Project [podcast_name_cleaned#702, value#699, regexp_replace(cleaned_value#720, [^a-zA-Z0-9\\s]+, , 1) AS cleaned_value#724]\n                                       +- Project [podcast_name_cleaned#702, value#699, regexp_replace(cleaned_value#716, \\b\\d+\\b, , 1) AS cleaned_value#720]\n                                          +- Project [podcast_name_cleaned#702, value#699, regexp_replace(cleaned_value#712, \\[\\d{2}:\\d{2}:\\d{2}\\]|\\d{2}:\\d{2}, , 1) AS cleaned_value#716]\n                                             +- Project [podcast_name_cleaned#702, value#699, lower(cleaned_value#708) AS cleaned_value#712]\n                                                +- Project [podcast_name_cleaned#702, value#699, concat_ws( , value#699) AS cleaned_value#708]\n                                                   +- Project [podcast_name_cleaned#702, value#699]\n                                                      +- Project [file_name#690, value#699, regexp_replace(regexp_replace(regexp_extract(file_name#690, /([^/]+?)(?:/[^/]+?\\.[a-z]+?$|$), 1), %20,  , 1), [^a-zA-Z0-9\\s], , 1) AS podcast_name_cleaned#702]\n                                                         +- Aggregate [file_name#690], [file_name#690, collect_list(value#688, 0, 0) AS value#699]\n                                                            +- Project [value#688, input_file_name() AS file_name#690]\n                                                               +- Relation [value#688] text\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-38f89ab992d6>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Find the index of the first occurrence of \"biden\" or \"trump\" in the filtered_tokens column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"biden_index\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfind_index_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"filtered_tokens\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"biden\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trump_index\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfind_index_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"filtered_tokens\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"trump\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   5172\u001b[0m                 \u001b[0mmessage_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"arg_name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"col\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"arg_type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5173\u001b[0m             )\n\u001b[0;32m-> 5174\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `biden` cannot be resolved. Did you mean one of the following? [`filtered_tokens`, `context_tokens`, `cleaned_transcript`, `podcast_name_cleaned`].;\n'Project [podcast_name_cleaned#702, cleaned_transcript#732, filtered_tokens#752, context_tokens#1282, <lambda>(filtered_tokens#752, 'biden)#1358 AS biden_index#1359]\n+- Project [podcast_name_cleaned#702, cleaned_transcript#732, filtered_tokens#752, <lambda>(filtered_tokens#752)#1281 AS context_tokens#1282]\n   +- Project [podcast_name_cleaned#702, cleaned_transcript#732, filtered_tokens#752, <lambda>(filtered_tokens#752)#1206 AS context_tokens#1207]\n      +- Project [podcast_name_cleaned#702, cleaned_transcript#732, filtered_tokens#752, <lambda>(filtered_tokens#752)#1131 AS context_tokens#1132]\n         +- Project [podcast_name_cleaned#702, cleaned_transcript#732, filtered_tokens#752, <lambda>(filtered_tokens#752)#1056 AS context_tokens#1057]\n            +- Project [podcast_name_cleaned#702, cleaned_transcript#732, filtered_tokens#752, <lambda>(filtered_tokens#752)#981 AS context_tokens#982]\n               +- Project [podcast_name_cleaned#702, cleaned_transcript#732, filtered_tokens#752, <lambda>(filtered_tokens#752)#906 AS context_tokens#907]\n                  +- Project [podcast_name_cleaned#702, cleaned_transcript#732, filtered_tokens#752, <lambda>(filtered_tokens#752)#831 AS context_tokens#832]\n                     +- Project [podcast_name_cleaned#702, cleaned_transcript#732, filtered_tokens#752]\n                        +- Project [podcast_name_cleaned#702, value#699, cleaned_value#728, cleaned_transcript#732, tokens#740, UDF(tokens#740) AS filtered_tokens#752]\n                           +- Project [podcast_name_cleaned#702, value#699, cleaned_value#728, cleaned_transcript#732, UDF(cleaned_transcript#732) AS tokens#740]\n                              +- Project [podcast_name_cleaned#702, value#699, cleaned_value#728, regexp_replace(cleaned_value#728, \\b\\d+m\\s*\\d+s|\\b\\d+s, , 1) AS cleaned_transcript#732]\n                                 +- Project [podcast_name_cleaned#702, value#699, trim(cleaned_value#724, None) AS cleaned_value#728]\n                                    +- Project [podcast_name_cleaned#702, value#699, regexp_replace(cleaned_value#720, [^a-zA-Z0-9\\s]+, , 1) AS cleaned_value#724]\n                                       +- Project [podcast_name_cleaned#702, value#699, regexp_replace(cleaned_value#716, \\b\\d+\\b, , 1) AS cleaned_value#720]\n                                          +- Project [podcast_name_cleaned#702, value#699, regexp_replace(cleaned_value#712, \\[\\d{2}:\\d{2}:\\d{2}\\]|\\d{2}:\\d{2}, , 1) AS cleaned_value#716]\n                                             +- Project [podcast_name_cleaned#702, value#699, lower(cleaned_value#708) AS cleaned_value#712]\n                                                +- Project [podcast_name_cleaned#702, value#699, concat_ws( , value#699) AS cleaned_value#708]\n                                                   +- Project [podcast_name_cleaned#702, value#699]\n                                                      +- Project [file_name#690, value#699, regexp_replace(regexp_replace(regexp_extract(file_name#690, /([^/]+?)(?:/[^/]+?\\.[a-z]+?$|$), 1), %20,  , 1), [^a-zA-Z0-9\\s], , 1) AS podcast_name_cleaned#702]\n                                                         +- Aggregate [file_name#690], [file_name#690, collect_list(value#688, 0, 0) AS value#699]\n                                                            +- Project [value#688, input_file_name() AS file_name#690]\n                                                               +- Relation [value#688] text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(\"context_tokens\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "DoO2uLY-zwKZ",
        "outputId": "7adb782b-7b41-45f1-a265-ad5af3332569"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "error",
          "ename": "PythonException",
          "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"<ipython-input-53-af22b2300dfe>\", line 17, in <lambda>\n  File \"<ipython-input-53-af22b2300dfe>\", line 10, in extract_context\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 174, in wrapped\n    return f(*args, **kwargs)\nTypeError: max() takes 1 positional argument but 2 were given\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-7b73745d9f10>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"context_tokens\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    943\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mBob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \"\"\"\n\u001b[0;32m--> 945\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m     def _show_string(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    974\u001b[0m                 )\n\u001b[1;32m    975\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_truncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"<ipython-input-53-af22b2300dfe>\", line 17, in <lambda>\n  File \"<ipython-input-53-af22b2300dfe>\", line 10, in extract_context\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 174, in wrapped\n    return f(*args, **kwargs)\nTypeError: max() takes 1 positional argument but 2 were given\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/Shareddrives/DATA228"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_42bwEOl0OyB",
        "outputId": "e60431e7-b843-4548-e63b-7e3e2c105a47"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/Shareddrives/DATA228\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/soniawmeyer/PodPresPred.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLliIaOjIh4u",
        "outputId": "eaad3d0e-db63-4bd0-b25f-dec3c736dfcd"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PodPresPred'...\n",
            "remote: Enumerating objects: 77, done.\u001b[K\n",
            "remote: Counting objects: 100% (77/77), done.\u001b[K\n",
            "remote: Compressing objects: 100% (67/67), done.\u001b[K\n",
            "remote: Total 77 (delta 26), reused 53 (delta 8), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (77/77), 557.40 KiB | 8.99 MiB/s, done.\n",
            "Resolving deltas: 100% (26/26), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/Shareddrives/DATA228/datacleaningexceptpickingouttokens.ipynb /content/drive/Shareddrives/DATA228/PodPresPred"
      ],
      "metadata": {
        "id": "V_mlq8VkIw6p"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/Shareddrives/DATA228/PodPresPred\n",
        "!git add .\n",
        "!git commit -m \"everything except the token picking on the text files.\"\n",
        "!git push origin main\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeHNlT0GJFSQ",
        "outputId": "d3714271-e3a1-4268-dfaa-7c04e5feb438"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/Shareddrives/DATA228/PodPresPred\n",
            "Author identity unknown\n",
            "\n",
            "*** Please tell me who you are.\n",
            "\n",
            "Run\n",
            "\n",
            "  git config --global user.email \"you@example.com\"\n",
            "  git config --global user.name \"Your Name\"\n",
            "\n",
            "to set your account's default identity.\n",
            "Omit --global to set the identity only in this repository.\n",
            "\n",
            "fatal: unable to auto-detect email address (got 'root@1a2dc8d9d7c1.(none)')\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"bhuvanck8@gmail.com\"\n",
        "!git config --global user.name \"Your Name\"\n",
        "!git add .\n",
        "!git commit -m \"data cleaning except 100 tokens is done on the text files\"\n",
        "!git push origin main\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWQcjxkSKYM7",
        "outputId": "edc0927c-b874-46d5-ae04-eb92a33e9717"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "fatal: Unable to read current working directory: No such file or directory\n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "fatal: Unable to read current working directory: No such file or directory\n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "fatal: Unable to read current working directory: No such file or directory\n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "fatal: Unable to read current working directory: No such file or directory\n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "fatal: Unable to read current working directory: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qUuNZ0SaOKDc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}